# enhanced_rag_comparison.py
import os
import mysql.connector
import torch
import pandas as pd
from typing import List, Dict, Any, Tuple
import re

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from transformers import BitsAndBytesConfig


class DatabaseKnowledgeExtractor:
    def __init__(self, db_config, model_path):
        self.db_config = db_config
        self.model_path = model_path
        self.conn = None
        self.cursor = None
        
    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        self.conn = mysql.connector.connect(**self.db_config)
        self.cursor = self.conn.cursor(dictionary=True)
        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
    
    def extract_schema_info(self):
        """æå–æ•°æ®åº“æ¨¡å¼ä¿¡æ¯"""
        schema_info = {
            "tables": {},
            "relationships": [],
            "statistics": {}
        }
        
        # è·å–æ‰€æœ‰è¡¨
        self.cursor.execute("SHOW TABLES")
        tables = [list(table.values())[0] for table in self.cursor.fetchall()]
        
        for table in tables:
            # è·å–è¡¨ç»“æ„
            self.cursor.execute(f"DESCRIBE {table}")
            columns = self.cursor.fetchall()
            
            # è·å–ç´¢å¼•ä¿¡æ¯
            self.cursor.execute(f"SHOW INDEX FROM {table}")
            indexes = self.cursor.fetchall()
            
            schema_info["tables"][table] = {
                "columns": columns,
                "indexes": indexes
            }
            
            # è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯
            self.cursor.execute(f"SELECT COUNT(*) as count FROM {table}")
            count_result = self.cursor.fetchone()
            schema_info["statistics"][table] = {
                "row_count": count_result["count"]
            }
        
        # æå–å¤–é”®å…³ç³»ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        schema_info["relationships"] = self._extract_relationships()
        
        return schema_info
    
    def _extract_relationships(self):
        """æå–è¡¨ä¹‹é—´çš„å…³ç³»"""
        relationships = []
        
        # åŸºäºå‘½åçº¦å®šå’Œæ•°æ®åˆ†ææ¨æ–­å…³ç³»
        table_pairs = [
            ("users", "orders", "user_id"),
            ("products", "order_items", "product_id"),
            ("orders", "order_items", "order_id"),
            ("categories", "products", "category_id")
        ]
        
        for table1, table2, key in table_pairs:
            relationships.append({
                "table1": table1,
                "table2": table2,
                "relationship": f"{table1}.{key} = {table2}.{key}",
                "type": "foreign_key"
            })
        
        return relationships
    
    def extract_sample_data(self, sample_size=5):
        """æå–æ ·æœ¬æ•°æ®ç”¨äºç†è§£æ•°æ®åˆ†å¸ƒ"""
        sample_data = {}
        
        tables = ["users", "products", "orders", "order_items", "categories"]
        
        for table in tables:
            try:
                self.cursor.execute(f"SELECT * FROM {table} LIMIT {sample_size}")
                sample_data[table] = self.cursor.fetchall()
            except:
                print(f"æ— æ³•è·å–è¡¨ {table} çš„æ ·æœ¬æ•°æ®")
        
        return sample_data

class EnhancedRAGComparison:
    def __init__(self, db_config, model_path):
        self.db_config = db_config
        self.model_path = model_path
        self.conn = None
        self.cursor = None
        
    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        self.conn = mysql.connector.connect(**self.db_config)
        self.cursor = self.conn.cursor(dictionary=True)
        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
    
    def get_challenging_test_scenarios(self):

        """åˆ›å»ºæ›´å…·æŒ‘æˆ˜æ€§çš„æµ‹è¯•åœºæ™¯"""
        return [          
            {
                "name": "åœºæ™¯1: ç®€å•å•è¡¨æŸ¥è¯¢",
                "question": "æŸ¥è¯¢å‰10ä¸ªç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯ï¼Œé™åˆ¶10ä¸ªè¾“å‡º",
                "expected_sql": "SELECT user_id, username, email, city, country FROM users LIMIT 10"
            },
            {
                "name": "åœºæ™¯2: å¸¦æ¡ä»¶çš„å•è¡¨æŸ¥è¯¢", 
                "question": "æŸ¥è¯¢æ¥è‡ªçº½çº¦çš„ç”¨æˆ·ï¼Œé™åˆ¶10ä¸ªè¾“å‡º",
                "expected_sql": "SELECT * FROM users WHERE city = 'New York' LIMIT 10"
            },
            {
                "name": "åœºæ™¯3: ä¸¤è¡¨è¿æ¥æŸ¥è¯¢",
                "question": "æŸ¥è¯¢äº§å“åŠå…¶åˆ†ç±»ä¿¡æ¯ï¼Œé™åˆ¶10ä¸ªè¾“å‡º",
                "expected_sql": "SELECT p.product_name, p.price, c.category_name FROM products p JOIN categories c ON p.category_id = c.category_id LIMIT 10"
            },
            {
                "name": "åœºæ™¯4: èšåˆæŸ¥è¯¢",
                "question": "ç»Ÿè®¡æ¯ä¸ªåŸå¸‚çš„ç”¨æˆ·æ•°é‡ï¼Œé™åˆ¶10ä¸ªè¾“å‡º",
                "expected_sql": "SELECT city, COUNT(*) as user_count FROM users GROUP BY city ORDER BY user_count DESC LIMIT 10"
            },
            {
                "name": "åœºæ™¯5: å¤æ‚å¤šè¡¨è¿æ¥",
                "question": "æŸ¥è¯¢æ¯ä¸ªç”¨æˆ·çš„è®¢å•æ€»é‡‘é¢ï¼Œé™åˆ¶10ä¸ªè¾“å‡º",
                "expected_sql": "SELECT u.username, SUM(o.total_amount) as total_spent FROM users u JOIN orders o ON u.user_id = o.user_id GROUP BY u.user_id, u.username ORDER BY total_spent DESC LIMIT 10"
            },
            # æ›´å…·æŒ‘æˆ˜æ€§çš„åœºæ™¯
            {
                "name": "åœºæ™¯6: å¤æ‚åˆ—åæŸ¥è¯¢",
                "question": "æŸ¥è¯¢ç”¨æˆ·çš„æ³¨å†Œæ—¥æœŸå’Œæœ€åç™»å½•æ—¶é—´ï¼Œæ˜¾ç¤ºç”¨æˆ·IDã€ç”¨æˆ·åå’ŒåŸå¸‚ï¼Œé™åˆ¶10ä¸ªè¾“å‡º",
                "expected_sql": "SELECT user_id, username, city, registration_date, last_login FROM users LIMIT 10",
                "challenge": "éœ€è¦çŸ¥é“å…·ä½“çš„æ—¥æœŸæ—¶é—´åˆ—å"
            },
            {
                "name": "åœºæ™¯7: å¤šè¡¨è¿æ¥ä¸ç‰¹å®šåˆ—", 
                "question": "æŸ¥è¯¢è®¢å•è¯¦æƒ…ï¼ŒåŒ…æ‹¬è®¢å•IDã€ç”¨æˆ·åã€äº§å“åç§°ã€æ•°é‡å’Œå•ä»·ï¼Œé™åˆ¶10ä¸ªè¾“å‡º",
                "expected_sql": """SELECT o.order_id, u.username, p.product_name, oi.quantity, oi.unit_price 
FROM orders o 
JOIN users u ON o.user_id = u.user_id 
JOIN order_items oi ON o.order_id = oi.order_id 
JOIN products p ON oi.product_id = p.product_id 
LIMIT 10""",
                "challenge": "éœ€è¦çŸ¥é“å››è¡¨è¿æ¥å’Œæ­£ç¡®çš„åˆ—å"
            },
            {
                "name": "åœºæ™¯8: èšåˆå‡½æ•°ä¸åˆ†ç»„",
                "question": "ç»Ÿè®¡æ¯ä¸ªäº§å“ç±»åˆ«çš„å¹³å‡ä»·æ ¼å’Œäº§å“æ•°é‡ï¼ŒæŒ‰å¹³å‡ä»·æ ¼é™åºæ’åˆ—ï¼Œé™åˆ¶10ä¸ªè¾“å‡º",
                "expected_sql": """SELECT c.category_name, 
AVG(p.price) as avg_price, 
COUNT(p.product_id) as product_count 
FROM products p 
JOIN categories c ON p.category_id = c.category_id 
GROUP BY c.category_id, c.category_name 
ORDER BY avg_price DESC""",
                "challenge": "éœ€è¦çŸ¥é“èšåˆå‡½æ•°å’Œåˆ†ç»„é€»è¾‘"
            },
            {
                "name": "åœºæ™¯9: å¤æ‚æ¡ä»¶æŸ¥è¯¢",
                "question": "æŸ¥è¯¢æœ€è¿‘30å¤©å†…æ³¨å†Œä¸”æ¥è‡ªç¾å›½çº½çº¦çš„é»„é‡‘ç­‰çº§ç”¨æˆ·ï¼Œé™åˆ¶10ä¸ªè¾“å‡º",
                "expected_sql": """SELECT user_id, username, email, city, loyalty_level, registration_date 
FROM users 
WHERE city = 'New York' 
AND country = 'USA' 
AND loyalty_level = 'Gold' 
AND registration_date >= DATE_SUB(CURDATE(), INTERVAL 30 DAY)""",
                "challenge": "éœ€è¦çŸ¥é“æ—¥æœŸå‡½æ•°å’Œå¤šä¸ªæ¡ä»¶"
            },
            {
                "name": "åœºæ™¯10: å­æŸ¥è¯¢ä¸é«˜çº§åˆ†æ",
                "question": "æŸ¥è¯¢æ¶ˆè´¹é‡‘é¢é«˜äºå¹³å‡æ¶ˆè´¹æ°´å¹³çš„ç”¨æˆ·åŠå…¶è®¢å•æ€»æ•°ï¼Œé™åˆ¶10ä¸ªè¾“å‡º",
                "expected_sql": """SELECT u.username, 
COUNT(o.order_id) as order_count, 
SUM(o.total_amount) as total_spent 
FROM users u 
JOIN orders o ON u.user_id = o.user_id 
GROUP BY u.user_id, u.username 
HAVING total_spent > (SELECT AVG(total_amount) FROM orders) 
ORDER BY total_spent DESC 
LIMIT 10""",
                "challenge": "éœ€è¦å­æŸ¥è¯¢å’ŒHAVINGå­å¥"
            }
        ]
    
    def execute_and_compare(self, sql_query, expected_sql, description):
        print(f"\nğŸ” {description}")
        print(f"ç”Ÿæˆçš„SQL:\n {sql_query}")
        # print(f"æœŸæœ›çš„SQL:\n {expected_sql}")
        
    


class AdvancedRAGSystem:
    def __init__(self, config, db_config):
        self.config = config
        self.db_config = db_config
        self.embeddings = self._init_embeddings()
        self.llm = self._init_llm()
        self.vector_db = None
        self.qa_chain = None
        
    def _init_embeddings(self):
        print("åŠ è½½bge-small-zh-v1.5åµŒå…¥æ¨¡å‹...")
        return HuggingFaceBgeEmbeddings(
            model_name=self.config.EMBEDDING_MODEL_NAME,
            # model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True},
            query_instruction="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
        )

    def _init_llm(self):
        print(f"åŠ è½½Qwen3æ¨¡å‹: {self.config.LLM_MODEL_NAME}")

        tokenizer = AutoTokenizer.from_pretrained(self.config.LLM_MODEL_NAME)

        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )

        model = AutoModelForCausalLM.from_pretrained(
            self.config.LLM_MODEL_NAME,
            quantization_config=quantization_config,  # ä½¿ç”¨æ–°çš„å‚æ•°
            # device_map="cuda",
            device_map="auto",
            dtype=torch.float16,
            trust_remote_code=True
        )

        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=1000,  # å¢åŠ tokenæ•°é‡å¤„ç†å¤æ‚SQL
            temperature=0.1,     # é™ä½éšæœºæ€§
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

        return HuggingFacePipeline(pipeline=pipe)
    
    def load_rag_system(self):
        """åŠ è½½RAGç³»ç»Ÿ"""
        if os.path.exists(self.config.VECTOR_DB_DIR):
            print("åŠ è½½å·²æœ‰å‘é‡åº“...")
            self.vector_db = Chroma(
                persist_directory=self.config.VECTOR_DB_DIR,
                embedding_function=self.embeddings
            )
            
            # SQLç”Ÿæˆæç¤ºæ¨¡æ¿
            sql_prompt_template = """ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ã€‚åŸºäºä»¥ä¸‹æ•°æ®åº“ç»“æ„çŸ¥è¯†å’Œç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆå‡†ç¡®ä¸”ä¼˜åŒ–çš„SQLæŸ¥è¯¢è¯­å¥ã€‚

æ•°æ®åº“ç»“æ„ä¿¡æ¯:
{context}

ç”¨æˆ·é—®é¢˜: {question}

è¯·éµå¾ªä»¥ä¸‹è§„åˆ™:
1. åªè¿”å›SQLæŸ¥è¯¢è¯­å¥ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Š
2. ä½¿ç”¨æ­£ç¡®çš„è¡¨åå’Œåˆ—å
3. åŒ…å«é€‚å½“çš„WHEREæ¡ä»¶ã€JOINæ¡ä»¶å’ŒGROUP BYå­å¥
4. å¯¹äºåˆ†é¡µæŸ¥è¯¢ä½¿ç”¨LIMIT
5. ç¡®ä¿SQLè¯­æ³•æ­£ç¡®

SQLæŸ¥è¯¢:"""
            
            sql_prompt = PromptTemplate(
                template=sql_prompt_template,
                input_variables=["context", "question"]
            )
            
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.vector_db.as_retriever(search_kwargs={"k": self.config.TOP_K}),
                chain_type_kwargs={"prompt": sql_prompt},
                return_source_documents=True
            )
            return True
        return False
    
    def generate_sql_without_rag(self, question: str) -> str:
        """ä¸ä½¿ç”¨RAGç”ŸæˆSQLï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰"""
        prompt = f"""è¯·ä¸ºä»¥ä¸‹é—®é¢˜ç”ŸæˆSQLæŸ¥è¯¢è¯­å¥ã€‚æ•°æ®åº“åŒ…å«users, products, orders, order_items, categoriesç­‰è¡¨ã€‚

é—®é¢˜: {question}

è¯·åªè¿”å›SQLæŸ¥è¯¢è¯­å¥:"""
        
        result = self.llm.invoke(prompt)
        return self._extract_sql_from_response(result)
    
    def generate_sql_with_rag(self, question: str) -> Tuple[str, List]:
        """ä½¿ç”¨RAGç”ŸæˆSQL"""
        result = self.qa_chain.invoke({"query": question}) 
        sql = self._extract_sql_from_response(result["result"])
        return sql, result["source_documents"]
    
    def _extract_sql_from_response(self, response: str) -> str:
        """ä»å“åº”ä¸­æå–SQLè¯­å¥"""
        # æŸ¥æ‰¾SQLå¼€å§‹
        sql_start = -1
        sql_keywords = ["SELECT", "INSERT", "UPDATE", "DELETE", "WITH"]
        
        for keyword in sql_keywords:
            idx = response.upper().find(keyword)
            if idx != -1 and (sql_start == -1 or idx < sql_start):
                sql_start = idx
        
        if sql_start != -1:
            # æå–åˆ°åˆ†å·æˆ–ç»“å°¾
            sql_end = response.find(';', sql_start)
            if sql_end == -1:
                sql_end = len(response)
            
            sql = response[sql_start:sql_end].strip()
            # æ¸…ç†å¯èƒ½çš„Markdownä»£ç å—
            sql = re.sub(r'^```sql\s*|\s*```$', '', sql, flags=re.IGNORECASE)
            return sql.strip()
        
        return response.strip()


def generate_enhanced_overview(schema_info, sample_data):
    """ç”Ÿæˆæ•°æ®åº“æ¦‚è§ˆ"""
    content = "æ•°æ®åº“æ¦‚è§ˆ\n"
    content += "=" * 60 + "\n\n"
    
    content += "è¡¨ç»Ÿè®¡:\n"
    for table_name, stats in schema_info["statistics"].items():
        col_count = len(schema_info["tables"][table_name]["columns"])
        content += f"- {table_name}: {col_count}åˆ—, {stats['row_count']}è¡Œæ•°æ®\n"
    
    content += "\nå…³é”®å…³ç³»:\n"
    for rel in schema_info["relationships"]:
        content += f"- {rel['table1']} â†” {rel['table2']} ({rel['relationship']})\n"
    
    content += "\næ•°æ®ç‰¹å¾:\n"
    # æ·»åŠ æ•°æ®åˆ†å¸ƒä¿¡æ¯
    if 'users' in sample_data:
        cities = [user['city'] for user in sample_data['users'] if 'city' in user]
        if cities:
            content += f"- ç”¨æˆ·åŸå¸‚åˆ†å¸ƒç¤ºä¾‹: {', '.join(set(cities))}\n"
    
    return content


def generate_detailed_table_doc(table_name, table_info, sample_data):
    """ç”Ÿæˆè¯¦ç»†çš„è¡¨æ–‡æ¡£"""
    content = f"è¡¨è¯¦ç»†æ–‡æ¡£: {table_name}\n"
    content += "=" * 60 + "\n\n"
    
    content += "åˆ—è¯¦ç»†ä¿¡æ¯:\n"
    for col in table_info["columns"]:
        content += f"- {col['Field']}: {col['Type']} | Null: {col['Null']} | Key: {col['Key']} | Default: {col['Default']}\n"
    
    content += "\nç´¢å¼•ä¿¡æ¯:\n"
    for idx in table_info["indexes"]:
        content += f"- {idx['Column_name']} ({idx['Index_type']}) - {'å”¯ä¸€' if idx['Non_unique'] == 0 else 'éå”¯ä¸€'}\n"
    
    if sample_data:
        content += f"\næ•°æ®ç¤ºä¾‹ ({len(sample_data)} è¡Œ):\n"
        df = pd.DataFrame(sample_data)
        content += df.to_string() + "\n"
    
    return content


def generate_query_patterns():
    """ç”ŸæˆæŸ¥è¯¢æ¨¡å¼æ–‡æ¡£"""
    content = "å¸¸ç”¨æŸ¥è¯¢æ¨¡å¼\n"
    content += "=" * 60 + "\n\n"
    
    patterns = [
        {
            "pattern": "ç®€å•å•è¡¨æŸ¥è¯¢",
            "description": "æŸ¥è¯¢ç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯",
            "sql": "SELECT user_id, username, email, city, country FROM users"
        },
        {
            "pattern": "å¸¦æ¡ä»¶çš„å•è¡¨æŸ¥è¯¢",
            "description": "æŸ¥è¯¢æ¥è‡ªçº½çº¦çš„ç”¨æˆ·",
            "sql": "SELECT * FROM users WHERE city = 'New York'"
        },
        {
            "pattern": "ä¸¤è¡¨è¿æ¥æŸ¥è¯¢", 
            "description": "æŸ¥è¯¢äº§å“åŠå…¶åˆ†ç±»ä¿¡æ¯",
            "sql": "SELECT p.product_name, p.price, c.category_name FROM products p JOIN categories c ON p.category_id = c.category_id"
        },
        {
            "pattern": "èšåˆæŸ¥è¯¢",
            "description": "ç»Ÿè®¡æ¯ä¸ªåŸå¸‚çš„ç”¨æˆ·æ•°",
            "sql": "SELECT city, COUNT(*) as user_count FROM users GROUP BY city ORDER BY user_count DESC"
        },
        
        {
            "pattern": "å¤æ‚å¤šè¡¨è¿æ¥",
            "description": "æŸ¥è¯¢æ¯ä¸ªç”¨æˆ·çš„è®¢å•æ€»é‡‘é¢",
            "sql": "SELECT u.username, SUM(o.total_amount) as total_spent FROM users u JOIN orders o ON u.user_id = o.user_id GROUP BY u.user_id, u.username ORDER BY total_spent DESC"
        },
        {
            "pattern": " å¤æ‚åˆ—åæŸ¥è¯¢",
            "description": "æŸ¥è¯¢ç”¨æˆ·çš„æ³¨å†Œæ—¥æœŸå’Œæœ€åç™»å½•æ—¶é—´ï¼Œæ˜¾ç¤ºç”¨æˆ·IDã€ç”¨æˆ·åå’ŒåŸå¸‚",
            "sql": "SELECT user_id, username, city, registration_date, last_login FROM users"
        },
        {
            "pattern": " å¤šè¡¨è¿æ¥ä¸ç‰¹å®šåˆ—", 
            "description": "æŸ¥è¯¢è®¢å•è¯¦æƒ…ï¼ŒåŒ…æ‹¬è®¢å•IDã€ç”¨æˆ·åã€äº§å“åç§°ã€æ•°é‡å’Œå•ä»·",
            "sql": """SELECT o.order_id, u.username, p.product_name, oi.quantity, oi.unit_price 
FROM orders o 
JOIN users u ON o.user_id = u.user_id 
JOIN order_items oi ON o.order_id = oi.order_id 
JOIN products p ON oi.product_id = p.product_id """
        },
        {
            "pattern": " èšåˆå‡½æ•°ä¸åˆ†ç»„",
            "description": "ç»Ÿè®¡æ¯ä¸ªäº§å“ç±»åˆ«çš„å¹³å‡ä»·æ ¼å’Œäº§å“æ•°é‡ï¼ŒæŒ‰å¹³å‡ä»·æ ¼é™åºæ’åˆ—",
            "sql": """SELECT c.category_name, 
AVG(p.price) as avg_price, 
COUNT(p.product_id) as product_count 
FROM products p 
JOIN categories c ON p.category_id = c.category_id 
GROUP BY c.category_id, c.category_name 
ORDER BY avg_price DESC"""
        },
        {
            "pattern": " å¤æ‚æ¡ä»¶æŸ¥è¯¢",
            "description": "æŸ¥è¯¢æœ€è¿‘30å¤©å†…æ³¨å†Œä¸”æ¥è‡ªç¾å›½çº½çº¦çš„é»„é‡‘ç­‰çº§ç”¨æˆ·",
            "sql": """SELECT user_id, username, email, city, loyalty_level, registration_date 
FROM users 
WHERE city = 'New York' 
AND country = 'USA' 
AND loyalty_level = 'Gold' 
AND registration_date >= DATE_SUB(CURDATE(), INTERVAL 30 DAY)"""
        },
        {
            "pattern": " å­æŸ¥è¯¢ä¸é«˜çº§åˆ†æ",
            "description": "æŸ¥è¯¢æ¶ˆè´¹é‡‘é¢é«˜äºå¹³å‡æ¶ˆè´¹æ°´å¹³çš„ç”¨æˆ·åŠå…¶è®¢å•æ€»æ•°",
            "sql": """SELECT u.username, 
COUNT(o.order_id) as order_count, 
SUM(o.total_amount) as total_spent 
FROM users u 
JOIN orders o ON u.user_id = o.user_id 
GROUP BY u.user_id, u.username 
HAVING total_spent > (SELECT AVG(total_amount) FROM orders) 
ORDER BY total_spent DESC """
        }
    ]
    
    for pattern in patterns:
        content += f"{pattern['pattern']}:\n"
        content += f"æè¿°: {pattern['description']}\n"
        content += f"SQLæ¨¡å¼: {pattern['sql']}\n\n"
    
    return content


def generate_business_logic():
    """ç”Ÿæˆä¸šåŠ¡é€»è¾‘æ–‡æ¡£"""
    content = "ä¸šåŠ¡é€»è¾‘å’Œè§„åˆ™\n"
    content += "=" * 60 + "\n\n"
    
    business_rules = [
        "ç”¨æˆ·å¿ è¯šåº¦ç­‰çº§: Bronze(é’é“œ) < Silver(ç™½é“¶) < Gold(é»„é‡‘) < Platinum(ç™½é‡‘)",
        "è®¢å•çŠ¶æ€æµè½¬: pending(å¾…å¤„ç†) â†’ confirmed(å·²ç¡®è®¤) â†’ shipped(å·²å‘è´§) â†’ delivered(å·²é€è¾¾)",
        "æ”¯ä»˜çŠ¶æ€: pending(å¾…æ”¯ä»˜) â†’ paid(å·²æ”¯ä»˜) â†’ refunded(å·²é€€æ¬¾)",
        "åº“å­˜é¢„è­¦: å½“stock_quantity <= min_stock_levelæ—¶è§¦å‘è¡¥è´§æé†’",
        "ç”¨æˆ·æ³¨å†Œ: registration_dateè®°å½•æ³¨å†Œæ—¶é—´ï¼Œlast_loginè®°å½•æœ€åç™»å½•æ—¶é—´"
    ]
    
    for rule in business_rules:
        content += f"â€¢ {rule}\n"
    
    return content


def create_enhanced_database_docs(db_config, output_dir):
    """åˆ›å»ºæ•°æ®åº“æ–‡æ¡£"""
    # extractor = DatabaseKnowledgeExtractor(db_config, "D:/Qwen/Qwen/Qwen3-8B")
    extractor = DatabaseKnowledgeExtractor(db_config, "../Qwen/Qwen3-8B")
    extractor.connect()
    
    # æå–æ›´è¯¦ç»†çš„ä¿¡æ¯
    schema_info = extractor.extract_schema_info()
    sample_data = extractor.extract_sample_data(10)  # æ›´å¤šæ ·æœ¬
    
    docs = []
    
    # 1. æ•°æ®åº“æ¦‚è§ˆ
    overview = generate_enhanced_overview(schema_info, sample_data)
    docs.append(("enhanced_overview.txt", overview))
    
    # 2. è¯¦ç»†çš„è¡¨æ–‡æ¡£ï¼ˆåŒ…å«æ•°æ®ç±»å‹å’Œçº¦æŸï¼‰
    for table_name in schema_info["tables"]:
        table_doc = generate_detailed_table_doc(table_name, schema_info["tables"][table_name], sample_data.get(table_name, []))
        docs.append((f"table_{table_name}_detailed.txt", table_doc))
    
    # 3. æŸ¥è¯¢æ¨¡å¼æ–‡æ¡£
    query_patterns = generate_query_patterns()
    docs.append(("query_patterns.txt", query_patterns))
    
    # 4. ä¸šåŠ¡é€»è¾‘æ–‡æ¡£
    business_logic = generate_business_logic()
    docs.append(("business_logic.txt", business_logic))
    
    # ä¿å­˜æ–‡æ¡£
    os.makedirs(output_dir, exist_ok=True)
    for filename, content in docs:
        with open(os.path.join(output_dir, filename), "w", encoding="utf-8") as f:
            f.write(content)
    
    print(f"âœ… æ•°æ®åº“æ–‡æ¡£å·²ç”Ÿæˆåˆ° {output_dir}")


class Config:
    # DOCUMENTS_DIR = "D:/Qwen/Qwen3/enhanced_database_docs"
    DOCUMENTS_DIR = "enhanced_database_docs"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 80
    EMBEDDING_MODEL_NAME = "BAAI/bge-small-zh-v1.5"  
    # LLM_MODEL_NAME = "D:/Qwen/Qwen/Qwen3-8B"  
    LLM_MODEL_NAME = "../Qwen/Qwen3-8B"
    VECTOR_DB_DIR = "vector_db_enhanced"
    TOP_K = 3


def main():
    # æ•°æ®åº“é…ç½®
    db_config = {
        'host': 'localhost',
        'user': 'root',
        'password': 'admin',
        'database': 'test_rag_mid'
    }
    
    config = Config()
    
    print("ğŸš€ å¼€å§‹RAGå¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºæ•°æ®åº“æ–‡æ¡£
    print("ğŸ“ ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºæ•°æ®åº“æ–‡æ¡£...")
    create_enhanced_database_docs(db_config, config.DOCUMENTS_DIR)
    
    # ç¬¬äºŒæ­¥ï¼šæ„å»ºRAGç³»ç»Ÿ
    print("\nğŸ”§ ç¬¬äºŒæ­¥ï¼šæ„å»ºRAGç³»ç»Ÿ...")
    rag_system = AdvancedRAGSystem(config, db_config)
    
    # å¤„ç†æ–‡æ¡£
    loader = DirectoryLoader(
        config.DOCUMENTS_DIR,
        glob="*.txt",
        loader_cls=TextLoader,
        loader_kwargs={"encoding": "utf-8"}
    )
    documents = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.CHUNK_SIZE,
        chunk_overlap=config.CHUNK_OVERLAP,
        separators=["\n\n", "\n", "ã€‚", "ï¼Œ", "ï¼›", "ã€", " ", ""]
    )
    texts = text_splitter.split_documents(documents)
    
    # åˆ›å»ºå‘é‡åº“
    rag_system.vector_db = Chroma.from_documents(
        documents=texts,
        embedding=rag_system.embeddings,
        persist_directory=config.VECTOR_DB_DIR
    )
    rag_system.vector_db.persist()
    
    # åŠ è½½RAGç³»ç»Ÿ
    rag_system.load_rag_system()
    
    # ç¬¬ä¸‰æ­¥ï¼šè¿è¡ŒæŒ‘æˆ˜æ€§æµ‹è¯•
    print("\nğŸ¯ ç¬¬ä¸‰æ­¥ï¼šè¿è¡ŒæŒ‘æˆ˜æ€§æµ‹è¯•åœºæ™¯...")
    comparison = EnhancedRAGComparison(db_config, config.LLM_MODEL_NAME)
    comparison.connect()
    
    test_scenarios = comparison.get_challenging_test_scenarios()
    results = []
    
    for scenario in test_scenarios:
        print(f"\n{'='*80}")
        print(f"æµ‹è¯•: {scenario['name']}")
        # print(f"æŒ‘æˆ˜: {scenario['challenge']}")
        print(f"é—®é¢˜: {scenario['question']}")
        
        # æ— RAGç”Ÿæˆ
        print("\n1. æ— RAGç”ŸæˆSQL...")
        sql_no_rag = rag_system.generate_sql_without_rag(scenario['question'])
        result_no_rag = comparison.execute_and_compare(
            sql_no_rag, scenario['expected_sql'], "æ— RAGç»“æœ"
        )
        
        # æœ‰RAGç”Ÿæˆ
        print("\n2. æœ‰RAGç”ŸæˆSQL...")
        sql_with_rag, source_docs = rag_system.generate_sql_with_rag(scenario['question'])
        result_with_rag = comparison.execute_and_compare(
            sql_with_rag, scenario['expected_sql'], "æœ‰RAGç»“æœ"
        )
        
        # # æ˜¾ç¤ºç›¸å…³æ–‡æ¡£
        # print(f"\nğŸ“š ç›¸å…³æ–‡æ¡£ç‰‡æ®µ:")
        # for i, doc in enumerate(source_docs[:2], 1):
        #     print(f"æ–‡æ¡£ {i}: {doc.page_content[:150]}...")
        

if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    main()