# enhanced_rag_dialogue_system.py
import os
import mysql.connector
import torch
import pandas as pd
from typing import List, Dict, Any, Tuple
import re

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from transformers import BitsAndBytesConfig


class DatabaseKnowledgeExtractor:
    def __init__(self, db_config, model_path):
        self.db_config = db_config
        self.model_path = model_path
        self.conn = None
        self.cursor = None
        
    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        self.conn = mysql.connector.connect(**self.db_config)
        self.cursor = self.conn.cursor(dictionary=True)
        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
    
    def extract_schema_info(self):
        """æå–æ•°æ®åº“æ¨¡å¼ä¿¡æ¯"""
        schema_info = {
            "tables": {},
            "relationships": [],
            "statistics": {}
        }
        
        # è·å–æ‰€æœ‰è¡¨
        self.cursor.execute("SHOW TABLES")
        tables = [list(table.values())[0] for table in self.cursor.fetchall()]
        
        for table in tables:
            # è·å–è¡¨ç»“æ„
            self.cursor.execute(f"DESCRIBE {table}")
            columns = self.cursor.fetchall()
            
            # è·å–ç´¢å¼•ä¿¡æ¯
            self.cursor.execute(f"SHOW INDEX FROM {table}")
            indexes = self.cursor.fetchall()
            
            schema_info["tables"][table] = {
                "columns": columns,
                "indexes": indexes
            }
            
            # è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯
            self.cursor.execute(f"SELECT COUNT(*) as count FROM {table}")
            count_result = self.cursor.fetchone()
            schema_info["statistics"][table] = {
                "row_count": count_result["count"]
            }
        
        # æå–å¤–é”®å…³ç³»ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        schema_info["relationships"] = self._extract_relationships()
        
        return schema_info
    
    def _extract_relationships(self):
        """æå–è¡¨ä¹‹é—´çš„å…³ç³»"""
        relationships = []
        
        # åŸºäºå‘½åçº¦å®šå’Œæ•°æ®åˆ†ææ¨æ–­å…³ç³»
        table_pairs = [
            ("users", "orders", "user_id"),
            ("products", "order_items", "product_id"),
            ("orders", "order_items", "order_id"),
            ("categories", "products", "category_id")
        ]
        
        for table1, table2, key in table_pairs:
            relationships.append({
                "table1": table1,
                "table2": table2,
                "relationship": f"{table1}.{key} = {table2}.{key}",
                "type": "foreign_key"
            })
        
        return relationships
    
    def extract_sample_data(self, sample_size=5):
        """æå–æ ·æœ¬æ•°æ®ç”¨äºç†è§£æ•°æ®åˆ†å¸ƒ"""
        sample_data = {}
        
        tables = ["users", "products", "orders", "order_items", "categories"]
        
        for table in tables:
            try:
                self.cursor.execute(f"SELECT * FROM {table} LIMIT {sample_size}")
                sample_data[table] = self.cursor.fetchall()
            except:
                print(f"æ— æ³•è·å–è¡¨ {table} çš„æ ·æœ¬æ•°æ®")
        
        return sample_data


class AdvancedRAGSystem:
    def __init__(self, config, db_config):
        self.config = config
        self.db_config = db_config
        self.embeddings = self._init_embeddings()
        self.llm = self._init_llm()
        self.vector_db = None
        self.qa_chain = None
        
    def _init_embeddings(self):
        print("åŠ è½½bge-small-zh-v1.5åµŒå…¥æ¨¡å‹...")
        return HuggingFaceBgeEmbeddings(
            model_name=self.config.EMBEDDING_MODEL_NAME,
            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
            encode_kwargs={'normalize_embeddings': True},
            query_instruction="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
        )

    def _init_llm(self):
        print(f"åŠ è½½Qwen3æ¨¡å‹: {self.config.LLM_MODEL_NAME}")

        tokenizer = AutoTokenizer.from_pretrained(self.config.LLM_MODEL_NAME)

        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )

        model = AutoModelForCausalLM.from_pretrained(
            self.config.LLM_MODEL_NAME,
            quantization_config=quantization_config,
            device_map="cuda",
            dtype=torch.float16,
            trust_remote_code=True
        )

        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=1000,
            temperature=0.1,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

        return HuggingFacePipeline(pipeline=pipe)
    
    def load_rag_system(self):
        """åŠ è½½RAGç³»ç»Ÿ"""
        if os.path.exists(self.config.VECTOR_DB_DIR):
            print("åŠ è½½å·²æœ‰å‘é‡åº“...")
            self.vector_db = Chroma(
                persist_directory=self.config.VECTOR_DB_DIR,
                embedding_function=self.embeddings
            )
            
            # SQLç”Ÿæˆæç¤ºæ¨¡æ¿
            sql_prompt_template = """ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ã€‚åŸºäºä»¥ä¸‹æ•°æ®åº“ç»“æ„çŸ¥è¯†å’Œç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆå‡†ç¡®ä¸”ä¼˜åŒ–çš„SQLæŸ¥è¯¢è¯­å¥ã€‚

æ•°æ®åº“ç»“æ„ä¿¡æ¯:
{context}

ç”¨æˆ·é—®é¢˜: {question}

è¯·éµå¾ªä»¥ä¸‹è§„åˆ™:
1. åªè¿”å›SQLæŸ¥è¯¢è¯­å¥ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Š
2. ä½¿ç”¨æ­£ç¡®çš„è¡¨åå’Œåˆ—å
3. åŒ…å«é€‚å½“çš„WHEREæ¡ä»¶ã€JOINæ¡ä»¶å’ŒGROUP BYå­å¥
4. å¯¹äºåˆ†é¡µæŸ¥è¯¢ä½¿ç”¨LIMIT
5. ç¡®ä¿SQLè¯­æ³•æ­£ç¡®

SQLæŸ¥è¯¢:"""
            
            sql_prompt = PromptTemplate(
                template=sql_prompt_template,
                input_variables=["context", "question"]
            )
            
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.vector_db.as_retriever(search_kwargs={"k": self.config.TOP_K}),
                chain_type_kwargs={"prompt": sql_prompt},
                return_source_documents=True
            )
            return True
        return False
    
    def generate_sql_without_rag(self, question: str) -> str:
        """ä¸ä½¿ç”¨RAGç”ŸæˆSQLï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰"""
        prompt = f"""è¯·ä¸ºä»¥ä¸‹é—®é¢˜ç”ŸæˆSQLæŸ¥è¯¢è¯­å¥ã€‚æ•°æ®åº“åŒ…å«users, products, orders, order_items, categoriesç­‰è¡¨ã€‚

é—®é¢˜: {question}

è¯·åªè¿”å›SQLæŸ¥è¯¢è¯­å¥:"""
        
        result = self.llm.invoke(prompt)
        return self._extract_sql_from_response(result)
    
    def generate_sql_with_rag(self, question: str) -> Tuple[str, List]:
        """ä½¿ç”¨RAGç”ŸæˆSQL"""
        result = self.qa_chain.invoke({"query": question}) 
        sql = self._extract_sql_from_response(result["result"])
        return sql, result["source_documents"]
    
    def _extract_sql_from_response(self, response: str) -> str:
        """ä»å“åº”ä¸­æå–SQLè¯­å¥"""
        # æŸ¥æ‰¾SQLå¼€å§‹
        sql_start = -1
        sql_keywords = ["SELECT", "INSERT", "UPDATE", "DELETE", "WITH"]
        
        for keyword in sql_keywords:
            idx = response.upper().find(keyword)
            if idx != -1 and (sql_start == -1 or idx < sql_start):
                sql_start = idx
        
        if sql_start != -1:
            # æå–åˆ°åˆ†å·æˆ–ç»“å°¾
            sql_end = response.find(';', sql_start)
            if sql_end == -1:
                sql_end = len(response)
            
            sql = response[sql_start:sql_end].strip()
            # æ¸…ç†å¯èƒ½çš„Markdownä»£ç å—
            sql = re.sub(r'^```sql\s*|\s*```$', '', sql, flags=re.IGNORECASE)
            return sql.strip()
        
        return response.strip()
    
    def execute_sql_query(self, sql_query: str) -> Tuple[bool, Any]:
        """æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶è¿”å›ç»“æœ"""
        try:
            conn = mysql.connector.connect(**self.db_config)
            cursor = conn.cursor(dictionary=True)
            cursor.execute(sql_query)
            result = cursor.fetchall()
            cursor.close()
            conn.close()
            return True, result
        except Exception as e:
            return False, str(e)


def generate_enhanced_overview(schema_info, sample_data):
    """ç”Ÿæˆæ•°æ®åº“æ¦‚è§ˆ"""
    content = "æ•°æ®åº“æ¦‚è§ˆ\n"
    content += "=" * 60 + "\n\n"
    
    content += "è¡¨ç»Ÿè®¡:\n"
    for table_name, stats in schema_info["statistics"].items():
        col_count = len(schema_info["tables"][table_name]["columns"])
        content += f"- {table_name}: {col_count}åˆ—, {stats['row_count']}è¡Œæ•°æ®\n"
    
    content += "\nå…³é”®å…³ç³»:\n"
    for rel in schema_info["relationships"]:
        content += f"- {rel['table1']} â†” {rel['table2']} ({rel['relationship']})\n"
    
    content += "\næ•°æ®ç‰¹å¾:\n"
    # æ·»åŠ æ•°æ®åˆ†å¸ƒä¿¡æ¯
    if 'users' in sample_data:
        cities = [user['city'] for user in sample_data['users'] if 'city' in user]
        if cities:
            content += f"- ç”¨æˆ·åŸå¸‚åˆ†å¸ƒç¤ºä¾‹: {', '.join(set(cities))}\n"
    
    return content


def generate_detailed_table_doc(table_name, table_info, sample_data):
    """ç”Ÿæˆè¯¦ç»†çš„è¡¨æ–‡æ¡£"""
    content = f"è¡¨è¯¦ç»†æ–‡æ¡£: {table_name}\n"
    content += "=" * 60 + "\n\n"
    
    content += "åˆ—è¯¦ç»†ä¿¡æ¯:\n"
    for col in table_info["columns"]:
        content += f"- {col['Field']}: {col['Type']} | Null: {col['Null']} | Key: {col['Key']} | Default: {col['Default']}\n"
    
    content += "\nç´¢å¼•ä¿¡æ¯:\n"
    for idx in table_info["indexes"]:
        content += f"- {idx['Column_name']} ({idx['Index_type']}) - {'å”¯ä¸€' if idx['Non_unique'] == 0 else 'éå”¯ä¸€'}\n"
    
    if sample_data:
        content += f"\næ•°æ®ç¤ºä¾‹ ({len(sample_data)} è¡Œ):\n"
        df = pd.DataFrame(sample_data)
        content += df.to_string() + "\n"
    
    return content


def generate_query_patterns():
    """ç”ŸæˆæŸ¥è¯¢æ¨¡å¼æ–‡æ¡£"""
    content = "å¸¸ç”¨æŸ¥è¯¢æ¨¡å¼\n"
    content += "=" * 60 + "\n\n"
    
    patterns = [
        {
            "pattern": "ç®€å•å•è¡¨æŸ¥è¯¢",
            "description": "æŸ¥è¯¢ç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯",
            "sql": "SELECT user_id, username, email, city, country FROM users"
        },
        {
            "pattern": "å¸¦æ¡ä»¶çš„å•è¡¨æŸ¥è¯¢",
            "description": "æŸ¥è¯¢æ¥è‡ªçº½çº¦çš„ç”¨æˆ·",
            "sql": "SELECT * FROM users WHERE city = 'New York'"
        },
        {
            "pattern": "ä¸¤è¡¨è¿æ¥æŸ¥è¯¢", 
            "description": "æŸ¥è¯¢äº§å“åŠå…¶åˆ†ç±»ä¿¡æ¯",
            "sql": "SELECT p.product_name, p.price, c.category_name FROM products p JOIN categories c ON p.category_id = c.category_id"
        },
        {
            "pattern": "èšåˆæŸ¥è¯¢",
            "description": "ç»Ÿè®¡æ¯ä¸ªåŸå¸‚çš„ç”¨æˆ·æ•°",
            "sql": "SELECT city, COUNT(*) as user_count FROM users GROUP BY city ORDER BY user_count DESC"
        },
        {
            "pattern": "å¤æ‚å¤šè¡¨è¿æ¥",
            "description": "æŸ¥è¯¢æ¯ä¸ªç”¨æˆ·çš„è®¢å•æ€»é‡‘é¢",
            "sql": "SELECT u.username, SUM(o.total_amount) as total_spent FROM users u JOIN orders o ON u.user_id = o.user_id GROUP BY u.user_id, u.username ORDER BY total_spent DESC"
        }
    ]
    
    for pattern in patterns:
        content += f"{pattern['pattern']}:\n"
        content += f"æè¿°: {pattern['description']}\n"
        content += f"SQLæ¨¡å¼: {pattern['sql']}\n\n"
    
    return content


def generate_business_logic():
    """ç”Ÿæˆä¸šåŠ¡é€»è¾‘æ–‡æ¡£"""
    content = "ä¸šåŠ¡é€»è¾‘å’Œè§„åˆ™\n"
    content += "=" * 60 + "\n\n"
    
    business_rules = [
        "ç”¨æˆ·å¿ è¯šåº¦ç­‰çº§: Bronze(é’é“œ) < Silver(ç™½é“¶) < Gold(é»„é‡‘) < Platinum(ç™½é‡‘)",
        "è®¢å•çŠ¶æ€æµè½¬: pending(å¾…å¤„ç†) â†’ confirmed(å·²ç¡®è®¤) â†’ shipped(å·²å‘è´§) â†’ delivered(å·²é€è¾¾)",
        "æ”¯ä»˜çŠ¶æ€: pending(å¾…æ”¯ä»˜) â†’ paid(å·²æ”¯ä»˜) â†’ refunded(å·²é€€æ¬¾)",
        "åº“å­˜é¢„è­¦: å½“stock_quantity <= min_stock_levelæ—¶è§¦å‘è¡¥è´§æé†’",
        "ç”¨æˆ·æ³¨å†Œ: registration_dateè®°å½•æ³¨å†Œæ—¶é—´ï¼Œlast_loginè®°å½•æœ€åç™»å½•æ—¶é—´"
    ]
    
    for rule in business_rules:
        content += f"â€¢ {rule}\n"
    
    return content


def create_enhanced_database_docs(db_config, output_dir):
    """åˆ›å»ºæ•°æ®åº“æ–‡æ¡£"""
    extractor = DatabaseKnowledgeExtractor(db_config, "D:/Qwen/Qwen/Qwen3-8B")
    extractor.connect()
    
    # æå–æ›´è¯¦ç»†çš„ä¿¡æ¯
    schema_info = extractor.extract_schema_info()
    sample_data = extractor.extract_sample_data(10)  # æ›´å¤šæ ·æœ¬
    
    docs = []
    
    # 1. æ•°æ®åº“æ¦‚è§ˆ
    overview = generate_enhanced_overview(schema_info, sample_data)
    docs.append(("enhanced_overview.txt", overview))
    
    # 2. è¯¦ç»†çš„è¡¨æ–‡æ¡£ï¼ˆåŒ…å«æ•°æ®ç±»å‹å’Œçº¦æŸï¼‰
    for table_name in schema_info["tables"]:
        table_doc = generate_detailed_table_doc(table_name, schema_info["tables"][table_name], sample_data.get(table_name, []))
        docs.append((f"table_{table_name}_detailed.txt", table_doc))
    
    # 3. æŸ¥è¯¢æ¨¡å¼æ–‡æ¡£
    query_patterns = generate_query_patterns()
    docs.append(("query_patterns.txt", query_patterns))
    
    # 4. ä¸šåŠ¡é€»è¾‘æ–‡æ¡£
    business_logic = generate_business_logic()
    docs.append(("business_logic.txt", business_logic))
    
    # ä¿å­˜æ–‡æ¡£
    os.makedirs(output_dir, exist_ok=True)
    for filename, content in docs:
        with open(os.path.join(output_dir, filename), "w", encoding="utf-8") as f:
            f.write(content)
    
    print(f"âœ… æ•°æ®åº“æ–‡æ¡£å·²ç”Ÿæˆåˆ° {output_dir}")


class Config:
    DOCUMENTS_DIR = "D:/Qwen/Qwen3/enhanced_database_docs"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 80
    EMBEDDING_MODEL_NAME = "BAAI/bge-small-zh-v1.5"  
    LLM_MODEL_NAME = "D:/Qwen/Qwen/Qwen3-8B"  
    VECTOR_DB_DIR = "vector_db_enhanced"
    TOP_K = 3


class DatabaseDialogueSystem:
    def __init__(self, config, db_config):
        self.config = config
        self.db_config = db_config
        self.rag_system = None
        self.history = []
        
    def initialize_system(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        print("ğŸš€ åˆå§‹åŒ–æ•°æ®åº“å¯¹è¯ç³»ç»Ÿ...")
        print("=" * 60)
        
        # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºæ•°æ®åº“æ–‡æ¡£ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not os.path.exists(self.config.DOCUMENTS_DIR) or not os.listdir(self.config.DOCUMENTS_DIR):
            print("ğŸ“ åˆ›å»ºæ•°æ®åº“æ–‡æ¡£...")
            create_enhanced_database_docs(self.db_config, self.config.DOCUMENTS_DIR)
        else:
            print("ğŸ“ æ•°æ®åº“æ–‡æ¡£å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º...")
        
        # ç¬¬äºŒæ­¥ï¼šæ„å»ºRAGç³»ç»Ÿ
        print("\nğŸ”§ æ„å»ºRAGç³»ç»Ÿ...")
        self.rag_system = AdvancedRAGSystem(self.config, self.db_config)
        
        # å¦‚æœå‘é‡åº“ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º
        if not os.path.exists(self.config.VECTOR_DB_DIR):
            print("åˆ›å»ºå‘é‡åº“...")
            loader = DirectoryLoader(
                self.config.DOCUMENTS_DIR,
                glob="*.txt",
                loader_cls=TextLoader,
                loader_kwargs={"encoding": "utf-8"}
            )
            documents = loader.load()
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.config.CHUNK_SIZE,
                chunk_overlap=self.config.CHUNK_OVERLAP,
                separators=["\n\n", "\n", "ã€‚", "ï¼Œ", "ï¼›", "ã€", " ", ""]
            )
            texts = text_splitter.split_documents(documents)
            
            # åˆ›å»ºå‘é‡åº“
            self.rag_system.vector_db = Chroma.from_documents(
                documents=texts,
                embedding=self.rag_system.embeddings,
                persist_directory=self.config.VECTOR_DB_DIR
            )
            self.rag_system.vector_db.persist()
        
        # åŠ è½½RAGç³»ç»Ÿ
        self.rag_system.load_rag_system()
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
    
    def process_query(self, question: str, use_rag: bool = True) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        print(f"\n{'='*80}")
        print(f"ğŸ“ ç”¨æˆ·é—®é¢˜: {question}")
        
        response = {
            "question": question,
            "sql_query": "",
            "execution_success": False,
            "execution_result": None,
            "error_message": "",
            "source_documents": [],
            "use_rag": use_rag
        }
        
        try:
            # ç”ŸæˆSQL
            if use_rag:
                sql_query, source_docs = self.rag_system.generate_sql_with_rag(question)
                response["source_documents"] = source_docs
            else:
                sql_query = self.rag_system.generate_sql_without_rag(question)
            
            response["sql_query"] = sql_query
            print(f"ğŸ” ç”Ÿæˆçš„SQL: {sql_query}")
            
            # æ‰§è¡ŒSQLï¼ˆå¦‚æœæ˜¯SELECTæŸ¥è¯¢ï¼‰
            if sql_query.strip().upper().startswith('SELECT'):
                success, result = self.rag_system.execute_sql_query(sql_query)
                response["execution_success"] = success
                if success:
                    response["execution_result"] = result
                    print(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(result)} è¡Œç»“æœ")
                else:
                    response["error_message"] = result
                    print(f"âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {result}")
            else:
                print("âš ï¸  éSELECTæŸ¥è¯¢ï¼Œè·³è¿‡æ‰§è¡Œ")
            
            # æ·»åŠ åˆ°å†å²è®°å½•
            self.history.append(response)
            
        except Exception as e:
            error_msg = f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            response["error_message"] = error_msg
            print(f"âŒ {error_msg}")
        
        return response
    
    def display_result(self, response: Dict[str, Any]):
        """æ˜¾ç¤ºæŸ¥è¯¢ç»“æœ"""
        print(f"\nğŸ“Š æŸ¥è¯¢ç»“æœ:")
        print(f"ä½¿ç”¨RAG: {'æ˜¯' if response['use_rag'] else 'å¦'}")
        print(f"SQLæŸ¥è¯¢: {response['sql_query']}")
        
        if response["execution_success"] and response["execution_result"]:
            result = response["execution_result"]
            if result:
                df = pd.DataFrame(result)
                print(f"\nğŸ“‹ æŸ¥è¯¢ç»“æœ ({len(result)} è¡Œ):")
                print(df.to_string(index=False))
            else:
                print("ğŸ“‹ æŸ¥è¯¢ç»“æœ: æ— æ•°æ®")
        elif response["error_message"]:
            print(f"âŒ é”™è¯¯: {response['error_message']}")
        
        # # æ˜¾ç¤ºç›¸å…³æ–‡æ¡£ï¼ˆå¦‚æœä½¿ç”¨RAGï¼‰
        # if response["use_rag"] and response["source_documents"]:
        #     print(f"\nğŸ“š ç›¸å…³å‚è€ƒæ–‡æ¡£:")
        #     for i, doc in enumerate(response["source_documents"][:2], 1):
        #         content_preview = doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
        #         print(f"æ–‡æ¡£ {i}: {content_preview}")
    
    def show_history(self):
        """æ˜¾ç¤ºæŸ¥è¯¢å†å²"""
        print(f"\nğŸ“œ æŸ¥è¯¢å†å² ({len(self.history)} æ¡):")
        for i, item in enumerate(self.history, 1):
            status = "âœ…" if item["execution_success"] else "âŒ"
            print(f"{i}. {status} {item['question']}")
    
    def run_dialogue(self):
        """è¿è¡Œå¯¹è¯ç³»ç»Ÿ"""
        print("\nğŸ¯ æ•°æ®åº“å¯¹è¯ç³»ç»Ÿå·²å¯åŠ¨ï¼")
        print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç³»ç»Ÿ")
        print("è¾“å…¥ 'history' æŸ¥çœ‹å†å²è®°å½•")
        print("è¾“å…¥ 'toggle' åˆ‡æ¢RAGæ¨¡å¼")
        print("è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
        
        use_rag = True
        
        while True:
            try:
                user_input = input("\nğŸ’¬ è¯·è¾“å…¥æ‚¨çš„æŸ¥è¯¢: ").strip()
                
                if user_input.lower() in ['quit', 'exit']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                elif user_input.lower() == 'history':
                    self.show_history()
                    continue
                elif user_input.lower() == 'toggle':
                    use_rag = not use_rag
                    print(f"ğŸ”„ RAGæ¨¡å¼å·²{'å¼€å¯' if use_rag else 'å…³é—­'}")
                    continue
                elif user_input.lower() == 'help':
                    self.show_help()
                    continue
                elif not user_input:
                    continue
                
                # å¤„ç†æŸ¥è¯¢
                response = self.process_query(user_input, use_rag)
                self.display_result(response)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}")
    
    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        help_text = """
ğŸ¤– æ•°æ®åº“å¯¹è¯ç³»ç»Ÿå¸®åŠ©

å¯ç”¨å‘½ä»¤:
â€¢ è¾“å…¥è‡ªç„¶è¯­è¨€é—®é¢˜ - ç³»ç»Ÿä¼šç”Ÿæˆå¹¶æ‰§è¡Œç›¸åº”çš„SQLæŸ¥è¯¢
â€¢ 'quit' æˆ– 'exit' - é€€å‡ºç³»ç»Ÿ
â€¢ 'history' - æŸ¥çœ‹æŸ¥è¯¢å†å²
â€¢ 'toggle' - åˆ‡æ¢RAGæ¨¡å¼ï¼ˆå¼€å¯/å…³é—­ï¼‰
â€¢ 'help' - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹é—®é¢˜:
â€¢ "æŸ¥è¯¢å‰10ä¸ªç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯"
â€¢ "ç»Ÿè®¡æ¯ä¸ªåŸå¸‚çš„ç”¨æˆ·æ•°é‡"
â€¢ "æŸ¥è¯¢æ¥è‡ªçº½çº¦çš„ç”¨æˆ·"
â€¢ "æŸ¥è¯¢äº§å“åŠå…¶åˆ†ç±»ä¿¡æ¯"
â€¢ "æŸ¥è¯¢æ¯ä¸ªç”¨æˆ·çš„è®¢å•æ€»é‡‘é¢"

RAGæ¨¡å¼:
â€¢ å¼€å¯æ—¶ï¼šç³»ç»Ÿä¼šå‚è€ƒæ•°æ®åº“æ–‡æ¡£ç”Ÿæˆæ›´å‡†ç¡®çš„SQL
â€¢ å…³é—­æ—¶ï¼šç³»ç»Ÿä»…åŸºäºé€šç”¨çŸ¥è¯†ç”ŸæˆSQL
        """
        print(help_text)


def main():
    # æ•°æ®åº“é…ç½®
    db_config = {
        'host': 'localhost',
        'user': 'root',
        'password': 'admin',
        'database': 'test_rag_mid'
    }
    
    config = Config()
    
    # åˆ›å»ºå¹¶è¿è¡Œå¯¹è¯ç³»ç»Ÿ
    dialogue_system = DatabaseDialogueSystem(config, db_config)
    dialogue_system.initialize_system()
    dialogue_system.run_dialogue()


if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    main()